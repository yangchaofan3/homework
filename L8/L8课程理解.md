***Thinking1 GBDT+LR原理***

> ​	Gradient Boosting Decision Tree是一种自动构造新特征的方法，比如能够从原有的20个特征构造出100甚至跟多的特征，然后把特征组合传入给线性分类器。
>
> ​	在CTR预估中，首先通过构建弱分类器的多棵回归决策树进行学习，GBDT每棵树学习的都是之前所有树结果和的残差，迭代方向是之前模型损失函数的梯度下降方向，采用串联的Boosting方式优化；LR用于对构造的新特征排序，按有用性输出。



**评价指标NE，Calibration，AUC**

> **NE:Normalized Cross-Entropy**  每次展现时预测得到的log loss平均值，除以对整个数据集的平均log loss值。
>
> ![NE](https://github.com/yangchaofan3/homework/blob/master/L8/NE.PNG)
>
> ​	其中p代表训练数据的平均经验CTR，即background CTR,NE对background CTR不敏感，NE数值越小预测效果越好。
>
> **Calibration**: 预估CTR除以真实CTR，即预测点击数与真实点击数的比值，越接近1预测效果越好
>
> **AUC**：ROC曲线下方面积，横轴FP Rate,纵轴TP Rate。
>
> ​			TP Rate = TP/(TP+FN)
>
> ​			FP Rate = FP/(FP+TN)



***Thinking2 Wide & Deep的模型结构，为什么能通过具备记忆和泛化能力***

> 类似于DeepFM，模型中Wide推荐主要针对于可解释性强的单个或两个特征关系分析，如用户对iem的点击和购买行为进行OneHot编码作为离散特征，但这些特征都需要人为设计。
>
> **Wide线性模型**：输入特征可以连续或离散
>
> <img src="https://github.com/yangchaofan3/homework/blob/master/L8/wide.PNG" alt="wide" style="zoom:80%;" />
>
> 而Deep推荐则是从深度学习的角度分析多维特征间关系，试图找出未被发掘的隐向量，但这些自动学习生成的隐向量的可解释性差。
>
> **Deep模型（DNN）**：连续特征或Embedding后的离散特征
>
> <img src="C:\Users\Ricardo\Desktop\deep.PNG" alt="deep" style="zoom:80%;" />
>
> DNN的塔形结构：embedding的特征首先需要dense到低维稠密向量再输入隐藏层，解决维度爆炸问题。
>
> 反向训练更新隐藏层，常用到激活函数ReLu：
>
> <img src="C:\Users\Ricardo\Desktop\relu.PNG" alt="relu" style="zoom:75%;" />
>
> 解释：下一层的结果可由上一层的结果和Deep模型的权重线性组合，再加上偏差后作为输入变量通过ReLu得到输出。
>
> **记忆能力**：学习item或feature之间的相关频率，在历史数据中探索相关性的可行性；
>
> **泛化（推理）能力**：基于相关性的传递探索过去没有出现过的特征组合。
>
> Wide&Deep可以结合线性模型的记忆能力和DNN模型的泛化能力，在训练过程中同时优化 两个模型的参数



***Thinking3 CTR预估中，使用FM与DNN结合有哪些结合的方式，代表模型有哪些？***

> 1. **DeepFM**：并行方式结合，低阶特征全用FM分析，高阶特征的交互作用全用DNN学习。
> 2. **NFM**：串行方式结合，将FM的结果作为DNN的输入。先由FM通过隐向量完成两两特征的组合工作（对位相乘相加作为交叉特征）并解决稀疏问题，，对于非线性特征和高阶特征的交叉同样使用DNN来学习，最后对两部分向量concatenate到DNN的隐藏层学习。



***Thinking4 Surprise工具中的baseline算法原理，BaselineOnly和KNNBaseline有什么区别？***

> **baseline算法原理**
>
> > 基于统计的基准预测线打分
> >
> > ![baseline](C:\Users\Ricardo\Desktop\baseline.PNG)
> >
> > bui为预测值，bu是用户对整体的偏差，bi是商品对整体的偏差。
> >
> > 算法：ALS或SGD
>
> KNNBaseline是在KNN基础上考虑了baseline，即用户偏差和item偏差
>
> ![knnbaseline](C:\Users\Ricardo\Desktop\knnbaseline.PNG)



***Thinking5 GBDT和RF什么区别？***

> GBDT采用boosting即串行的学习方式，每棵树都是一个弱分类器，每次学习的都是之前所有树预测效果的残差，最后多个弱分类器的结果共同用于决策。
>
> RF采用bagging即并行的学习方式，多棵树同时从根节点开始学习，采取少数服从多数的原则从最后的学习结果中对比选取最好的一棵树作为分类器，最后只有一个强分类器用于决策。

***Thinking6 基于领域的协同过滤有哪些算法，原理是什么***

> 1. **KNNBasic**
>
>    > ![knnbasic](C:\Users\Ricardo\Desktop\knnbasic.PNG)  
>    >
>    > 用户u对物品i的兴趣程度度，等价于K个邻居对物品i的兴趣度。通过用户u、v间的相似度和v对商品i的兴趣度传递u对i的兴趣度
>    >
>    > 基于物品协同过滤：上面用户和商品换位
>
> 
>
> 2. **KNNWithMeans**
>
>    > ![knnbasicwithmeans](C:\Users\Ricardo\Desktop\knnbasicwithmeans.PNG)
>    >
>    > 在KNNBasic的基础上考虑用户均值影响，uv表示用户v的平均兴趣。
>
> 3. **KNNWithZscore**
>
>    > ![knnbasicwithzscore](C:\Users\Ricardo\Desktop\knnbasicwithzscore.PNG)
>    >
>    > 在KNNBasic的基础上考虑用户v的平均兴趣及标准差。
>
> 4. **KNNBaseline**
>
>    > ![knnbaseline](C:\Users\Ricardo\Desktop\knnbaseline.PNG)
>    >
>    > 在KNNBasic基础上考虑了baseline，即用户偏差和item偏差。
