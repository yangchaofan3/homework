1.  **FM算法的原理**

> ​	用线性回归的方式建立特征向量和目标之间的函数关系，同时考虑了两个特征变量之间交互影响的二阶特征组合，如性别和年龄结合还能构造出“大妈”、“少女”之类的新特征，这些特征也有意义。对每两个特征之间的影响力Wij预估又回到了MF问题。

2. **FM算法的复杂度**

> ​	直接对Wij计算的复杂度为**O（k*n^2**）:n是特征个数（隐向量），k是特征的embedding size。
>
> 可以通过公式变换【*上三角/下三角=（全部-对角线)/2*】降维，把复杂度变成**O（k*n）**。



***Thinking1 实际工作中FM和MF哪个应用更多，为什么？*** 

> ​	FM用得更多，因为实际分析过程中MF具有局限性，只考虑两个特征如User和Item，而实际上预测问题包含的特征维度可能很多，MF处理不了多维信息。FM通过onehot编码建立庞大稀疏矩阵，能够学习分析多维数据的特征以及特征间相互作用的信息。
>
> **FM和MF的区别**
>
> > · FM把User和·Item进行了onehot编码，矩阵的特征维度庞大稀疏
> >
> > · MF是FM特例，只有UserID和Item ID的FM模型
> >
> > · MF只适用评分预测、简单特征计算，无法利用其他特征
> >
> > ·FM引入更多辅助信息作为特征：商品ID以外的品类，商店位置
> >
> > ·FM在计算二阶特征组合系数时用到了MF
>
> **FM在推荐系统的作用**
>
> FM在考虑单个因素对决策变量影响程度的同时，能考虑更高维度的变量间交互作用对决策变量的影响，实际运用中可以运用到回归问题如评分预测和二分类问题（sigmod）

3. **FFM算法原理**

> FM+Field aware
>
> 把同属于一个类别的属性放到一个field属性下
>
> 考虑了单个item和相关Field的关系，从一个维度更细化拆分到多个维度
>
> 采用FFM原因：更精细化；
>
>  改变了W的计算方式：每一个都有feature对应的field

4.**基于内容的推荐系统**

> 基于商品标签（分词jieba，NLTK）来进行推荐，计算当前商品特征向量与整个备选集特征矩阵的余弦相似度，推荐相似度top-N的商品。
>
> 用户依赖性低，冷启动有优势

5.**N-gram**

> N元语法：基于假设--第n个词出现与前n-1个词相关而与其他任何词不习惯
>
> 分词的一种方式：unigram，bigram，trigram

6.**余弦相似度**

> 通过测量两个向量夹角余弦值来度量它们之间的相似性：方向相同时相似度为1，夹角90度时相似度为0，方向相反时夹角-1。

7.**CountVectorizer和TfidfVectorizer**

> **CountVectorizer**：将文本中的词语转换为统计词频矩阵的工具，计算余弦相似度前的准备工作
>
> **TfidfVectorizer**：在CountVectorizer基础上结合TF-IDF的思想来提取文本特征，即对所有特征值进行tf-idf训练，按照min_df和max_df提取关键词并生成TF-IDF矩阵。

8.**Word Embedding**

>   一种降维方式，将不同特征向量one-hot后的高维降为固定维度的向量，使得对所有item 之间都能计算相似度。
>
> 还可以基于向量进行运算，如king-man+woman的向量结果和Queen更接近

9.**Word2Vec**

> 通过embedding，把原词所在的稀疏空间映射到一个新的空间中去，使得语义上相似的单词在该空间内距离相近，通过word embedding学习隐藏层的权重矩阵。
>
> word2vec实际上是一个查找表，对于输入的one-hot编码找到对应的隐藏层神经元节点，隐藏层的输出就是每个单词的word embedding。
>
> **两种模式**
>
> > Skip-Gram:给定input word预测上下文
> >
> > CBOW:给定上下文预测input word

***Thinking2 FFM与FM的区别*** 

> 1. FM每个特征只有一个隐向量，而FFM每个特征有多个隐向量，和不同向量点乘用到的隐向量不同，FM可以看成FFM的特例；
>
> 2. FFM采用了更多的隐向量，且每个隐向量都与field相关不能化简，因此不能降维，计算量比FM更大，O（k*n^2），更精细化。由于每种隐向量都要多次拆分，FFM的隐向量类别K一般远小于FM；
>
> 3. 特征转化后的格式不同，  FM特征格式-- label feat1:val1 feat2:val2                                                          											FFM特征格式-- label  field_id:feat_id:value 

***Thinking3 DeepFM相比于FM解决了哪些问题，原理是什么***

> FM特征组合过程受限于计算量一般只考虑2阶特征组合，而DeepFM解决了在FM基础上对更高阶特征组合的分析，对于低阶（1、2）特征才用FM分析，高阶特征处理采用DNN深度学习分析。
>
> 模型原理：把低阶FM的预测结果y值和高阶DNN预测结果y值线性组合，再用sigmod函数分类。
>
> 需要降维，输入模型前把稀疏矩阵稠密化固定维数

***Thinking4 小说网站向用户推荐某部小说相关的其他小说。原理和步骤是怎样的***

> 原理：越相似的小说用词越接近，特征向量的余弦相似度越高
>
> 1.  对小说的摘要描述进行分词，采用N-Gram方式提取关键词作为特征，利用TF-IDF把特征矩阵转为TF-IDF矩阵。
> 2. 计算所有小说之间的余弦相似度，生成相似度矩阵。
> 3. 对于在矩阵中选择已读小说的相似度向量，选择相似度最大的Top-N本小说进行推荐。

***Thinking5 Word2Vec在NLP和推荐系统的应用场景***

> NLP:将不同词汇出现的频率抽取并转换到固定大小的向量空间里，使不同essay之间的关系可以通过word特征关系的相似度来表示，还可以进行通过向量的线性计算进行比较。
>
> 推荐系统：在和NLP相同原理的基础上增加场景运用。例如社交网络用户关注的每一个大V都可以看作一个单词，关注大V的顺序列表看作essay，通过计算不同用户间essay的相似度匹配相似度最高的top-N用户，形成这些用户之间的推荐资源互换。在商品推荐中，用户已购买或收藏的商品是word，列表是essay，计算essay的余弦相似度可以用于推荐。
>
> 

