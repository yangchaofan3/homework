***Thinking1 机器学习中的监督学习、非监督学习、强化学习有何区别***

> 监督学习有label标签引导模型学习，如分类问题；非监督学习无label标签，如聚类问题。
>
> 强化学习与监督学习的区别：强化学习没有监督的label，奖励延后给出，如AlphaGo以最终获胜结果作为奖励反馈；监督学习则是即时给出。
>
> 强化学习与非监督学习的区别：强化学习有奖励，数据有前后依赖关系因此当前行为影响后续数据接收，注重时序；无监督l没有奖励。

***Thinking2 什么是策略网络，价值网络，有何区别***

> ​	价值网络通过计算整个数值网络上每一个状态的累积分数期望，期望数值越大代表状态奖励期望更多，最后从状态集选择最优期望作为下一步行动。
>
> ​	策略网络输出每一步对最后获得奖励的概率分布，最后选取概率最大的点作为下一步行动。
>
> ​	区别：价值网络注重当前局面的价值，以局部最优作为对终局最优的估计；策略网络从全局角度思考，寻求对最终取得奖励最有利的行动。

***Thinking3 请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select,  Expansion，Simluation，Backpropagation是如何操作的***

> ​	MCTS是一种有效减少搜索空间的启发式搜索算法，结合随机模拟的一般性和树搜索的准确性，在每个树节点展开的模拟实验中寻求比之前更优而不是最优结果作为子节点，通过模拟输出结果得分。
>
> ​	Select：从根节点按一定策略搜索到叶子节点，基于EE策略判断节点的价值，c越大越鼓励利用，c越小越鼓励探索。
>
> ​	Expansion：对叶子节点扩展一个或多个合法的子节点
>
> ​	Simulation：对子节点采用蒙特卡洛随机方式多次模拟实验，模拟的最终状态作为当前分数
>
> ​	Backpropagation：将子节点当前的分数向所有祖先节点依次回传更新。

***Thinking4 假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑***

> ​    强化学习在抖音可以广泛运用到视频推荐的主业务中，基于用户浏览时长和频次等构建奖励函数来学习用户的兴趣爱好，同时也需要考虑EE问题，一方面强调利用已明确的用户喜好推送培养粘性，另一方面以8：2或9：1的形式推送其他风格视频挖掘用户新的兴趣点。使用强化学习需要重点考虑用户反馈，由于推荐系统不同于下棋博弈有明确胜负，如何根据用户反馈内容制定合适的奖励函数关系到推荐内容的有效性。除浏览时长和频次外还有收藏点赞，结合NLP提取评论标签等方式值得考虑。

***Thinking5 在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路***

> ​	自动驾驶强化学习可以借鉴Flappy bird如何用AI控制点击从而控制向上还是向下的思路，基于策略网络来进行训练，车辆位置作为状态集，前后左右方向作为行动集，奖励分布则同时由向目标点的距离减少和不与外界碰撞来构造，由于车辆状态的迁移具有连续性，需要不断迭代更新车辆从当前位置到附近位置点的迁移概率，最后给出即时的最优策略。